{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cna3EKjoCJO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uO3Ke1ZbBKb2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5Fv1GGgBdPP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSClGUPICLzq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "r2sREPU5DCE0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of special characters and numbers\n",
        "special_characters_numbers = ['0', '1', '2', '3', '4','5', '6', '7', '8', '9',\n",
        "                              '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '{', '}', '[', ']',\n",
        "                              '|', '\\\\', ':', ';', '\"', '\\'', '<', '>', ',', '.', '/', '?']\n",
        "\n",
        "# Create a list of Vietnamese characters\n",
        "vietnamese_characters = ['a', 'á', 'à', 'ả', 'ã', 'ạ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ',\n",
        "                         'b', 'c', 'd', 'đ', 'e', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'g', 'h',\n",
        "                         'i', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'k', 'l', 'm', 'n', 'o', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ',\n",
        "                         'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'p', 'q', 'r', 's', 't', 'u', 'ú', 'ù', 'ủ', 'ũ',\n",
        "                         'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'v', 'x', 'y', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ']\n",
        "\n",
        "def remove_non_vietnamese_characters(text):\n",
        "    # Remove characters not in the valid Vietnamese characters list\n",
        "    cleaned_text = ''.join(char for char in text if char in vietnamese_characters)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_non_vietnamese_characters_return_specials(text):\n",
        "    # Remove characters not in the valid Vietnamese characters list\n",
        "    cleaned_text = ''.join(char for char in text if char in vietnamese_characters + ['-'])\n",
        "    special_chars = ''.join(char for char in text if char not in vietnamese_characters + ['-'])\n",
        "    return [cleaned_text, special_chars]"
      ],
      "metadata": {
        "id": "s5IZryPXDG5y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/truyen_kieu.txt') as f:\n",
        "  content = f.readlines()\n",
        "content = [line.strip('0123456789. \\n').lower() for line in content]\n",
        "content = ' '.join(content)\n",
        "content = content.split(' ')\n",
        "\n",
        "words = [remove_non_vietnamese_characters(word) for word in content]"
      ],
      "metadata": {
        "id": "HwhFZPJcDV6N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_truyen_ngan(file_path):\n",
        "  with open('/content/tat_den.txt') as f:\n",
        "    content = f.readlines()\n",
        "\n",
        "  content = ' '.join(content).lower()\n",
        "  content = content.split(' ')\n",
        "\n",
        "  # Dùng map để áp dụng remove_non_vietnamese_characters_return_specials lên tất cả các từ\n",
        "  words2 = list(map(remove_non_vietnamese_characters_return_specials, content))\n",
        "\n",
        "  # Convert the list of lists to a flat list\n",
        "  words2 = [word for sublist in words2 for word in sublist]\n",
        "\n",
        "  # Dùng list comprehension để loại bỏ những từ rỗng\n",
        "  words2 = [word for word in words2 if word != '']\n",
        "\n",
        "  return words2"
      ],
      "metadata": {
        "id": "dEUMPXWEDdfe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "truyen_ngan_paths = ['/content/tho_ho_xuan_huong.txt',\n",
        "                     '/content/top_bai_tho.txt',\n",
        "                     '/content/lao_hac.txt',\n",
        "                     '/content/tat_den.txt']"
      ],
      "metadata": {
        "id": "Rn-j7zuMDfAv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for truyen_ngan in truyen_ngan_paths:\n",
        "  words2 = read_truyen_ngan(truyen_ngan)\n",
        "  words = words + words2"
      ],
      "metadata": {
        "id": "GCzYO2tADgYl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = words + vietnamese_characters\n",
        "words = words + special_characters_numbers\n",
        "words = words + [' ', '\\n']\n",
        "\n",
        "words = [word for word in words if word != '']"
      ],
      "metadata": {
        "id": "9OSOEKzADhwg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3-i8ifiDjxU",
        "outputId": "c656bbcf-9ab7-46e6-ee8c-b612b536632c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192985"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global words_unique\n",
        "words_unique = sorted(list(set(words))) # Unique words in the dataset\n",
        "word_to_int = dict((c, i) for i, c in enumerate(words_unique)) # A dictionary to map unique words to integers. When we call enumerate function, we will get something like this [(0, 'A'), (1, 'B'), (2, 'C'), (3, 'D')]\n",
        "int_to_word = dict((i, c) for c, i in word_to_int.items()) # A dictionary to transform integers back to characters. This is just reversing of char_to_int."
      ],
      "metadata": {
        "id": "TVEqWBCsDlJ7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = len(words)\n",
        "n_vocab = len(words_unique)\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_words - 30, 1):\n",
        "    seq_in = words[i:i + 30]\n",
        "    seq_out = words[i + 30]\n",
        "    dataX.append([word_to_int[word] for word in seq_in])\n",
        "    dataY.append(word_to_int[seq_out])"
      ],
      "metadata": {
        "id": "ihVru9hIDm5L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_sequence = len(dataX)\n",
        "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_sequence, 30, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(dataY)"
      ],
      "metadata": {
        "id": "X_ipwDx8Dovj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save list and dictionary to file\n",
        "with open('/content/myTokenizer.pkl', 'wb') as file:\n",
        "    pickle.dump(words_unique, file)\n",
        "    pickle.dump(word_to_int, file)\n",
        "    pickle.dump(int_to_word, file)\n",
        "    pickle.dump(n_vocab, file)"
      ],
      "metadata": {
        "id": "78FpAuYVDqoS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :] # X is the output of LSTM, it will have this shape [batch_size, seq_len, hidden_size]. We only need to get the last value of the output sequence so we use -1.\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x) # Predicting one of the 46 characters in the vocab\n",
        "        return x"
      ],
      "metadata": {
        "id": "4Vpij1hhDznr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=128)\n",
        "model.train()\n",
        "for epoch in range(1000): # This will take a while\n",
        "    print(\"Running Epoch %d ...\" % epoch)\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print(\"Finished training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-nn-7o6EBf6",
        "outputId": "a0f951e4-ea94-4f31-cd7e-6ed51ca87f2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Epoch 0 ...\n",
            "Running Epoch 1 ...\n",
            "Running Epoch 2 ...\n",
            "Running Epoch 3 ...\n",
            "Running Epoch 4 ...\n",
            "Running Epoch 5 ...\n",
            "Running Epoch 6 ...\n",
            "Running Epoch 7 ...\n",
            "Running Epoch 8 ...\n",
            "Running Epoch 9 ...\n",
            "Running Epoch 10 ...\n",
            "Running Epoch 11 ...\n",
            "Running Epoch 12 ...\n",
            "Running Epoch 13 ...\n",
            "Running Epoch 14 ...\n",
            "Running Epoch 15 ...\n",
            "Running Epoch 16 ...\n",
            "Running Epoch 17 ...\n",
            "Running Epoch 18 ...\n",
            "Running Epoch 19 ...\n",
            "Running Epoch 20 ...\n",
            "Running Epoch 21 ...\n",
            "Running Epoch 22 ...\n",
            "Running Epoch 23 ...\n",
            "Running Epoch 24 ...\n",
            "Running Epoch 25 ...\n",
            "Running Epoch 26 ...\n",
            "Running Epoch 27 ...\n",
            "Running Epoch 28 ...\n",
            "Running Epoch 29 ...\n",
            "Running Epoch 30 ...\n",
            "Running Epoch 31 ...\n",
            "Running Epoch 32 ...\n",
            "Running Epoch 33 ...\n",
            "Running Epoch 34 ...\n",
            "Running Epoch 35 ...\n",
            "Running Epoch 36 ...\n",
            "Running Epoch 37 ...\n",
            "Running Epoch 38 ...\n",
            "Running Epoch 39 ...\n",
            "Running Epoch 40 ...\n",
            "Running Epoch 41 ...\n",
            "Running Epoch 42 ...\n",
            "Running Epoch 43 ...\n",
            "Running Epoch 44 ...\n",
            "Running Epoch 45 ...\n",
            "Running Epoch 46 ...\n",
            "Running Epoch 47 ...\n",
            "Running Epoch 48 ...\n",
            "Running Epoch 49 ...\n",
            "Running Epoch 50 ...\n",
            "Running Epoch 51 ...\n",
            "Running Epoch 52 ...\n",
            "Running Epoch 53 ...\n",
            "Running Epoch 54 ...\n",
            "Running Epoch 55 ...\n",
            "Running Epoch 56 ...\n",
            "Running Epoch 57 ...\n",
            "Running Epoch 58 ...\n",
            "Running Epoch 59 ...\n",
            "Running Epoch 60 ...\n",
            "Running Epoch 61 ...\n",
            "Running Epoch 62 ...\n",
            "Running Epoch 63 ...\n",
            "Running Epoch 64 ...\n",
            "Running Epoch 65 ...\n",
            "Running Epoch 66 ...\n",
            "Running Epoch 67 ...\n",
            "Running Epoch 68 ...\n",
            "Running Epoch 69 ...\n",
            "Running Epoch 70 ...\n",
            "Running Epoch 71 ...\n",
            "Running Epoch 72 ...\n",
            "Running Epoch 73 ...\n",
            "Running Epoch 74 ...\n",
            "Running Epoch 75 ...\n",
            "Running Epoch 76 ...\n",
            "Running Epoch 77 ...\n",
            "Running Epoch 78 ...\n",
            "Running Epoch 79 ...\n",
            "Running Epoch 80 ...\n",
            "Running Epoch 81 ...\n",
            "Running Epoch 82 ...\n",
            "Running Epoch 83 ...\n",
            "Running Epoch 84 ...\n",
            "Running Epoch 85 ...\n",
            "Running Epoch 86 ...\n",
            "Running Epoch 87 ...\n",
            "Running Epoch 88 ...\n",
            "Running Epoch 89 ...\n",
            "Running Epoch 90 ...\n",
            "Running Epoch 91 ...\n",
            "Running Epoch 92 ...\n",
            "Running Epoch 93 ...\n",
            "Running Epoch 94 ...\n",
            "Running Epoch 95 ...\n",
            "Running Epoch 96 ...\n",
            "Running Epoch 97 ...\n",
            "Running Epoch 98 ...\n",
            "Running Epoch 99 ...\n",
            "Running Epoch 100 ...\n",
            "Running Epoch 101 ...\n",
            "Running Epoch 102 ...\n",
            "Running Epoch 103 ...\n",
            "Running Epoch 104 ...\n",
            "Running Epoch 105 ...\n",
            "Running Epoch 106 ...\n",
            "Running Epoch 107 ...\n",
            "Running Epoch 108 ...\n",
            "Running Epoch 109 ...\n",
            "Running Epoch 110 ...\n",
            "Running Epoch 111 ...\n",
            "Running Epoch 112 ...\n",
            "Running Epoch 113 ...\n",
            "Running Epoch 114 ...\n",
            "Running Epoch 115 ...\n",
            "Running Epoch 116 ...\n",
            "Running Epoch 117 ...\n",
            "Running Epoch 118 ...\n",
            "Running Epoch 119 ...\n",
            "Running Epoch 120 ...\n",
            "Running Epoch 121 ...\n",
            "Running Epoch 122 ...\n",
            "Running Epoch 123 ...\n",
            "Running Epoch 124 ...\n",
            "Running Epoch 125 ...\n",
            "Running Epoch 126 ...\n",
            "Running Epoch 127 ...\n",
            "Running Epoch 128 ...\n",
            "Running Epoch 129 ...\n",
            "Running Epoch 130 ...\n",
            "Running Epoch 131 ...\n",
            "Running Epoch 132 ...\n",
            "Running Epoch 133 ...\n",
            "Running Epoch 134 ...\n",
            "Running Epoch 135 ...\n",
            "Running Epoch 136 ...\n",
            "Running Epoch 137 ...\n",
            "Running Epoch 138 ...\n",
            "Running Epoch 139 ...\n",
            "Running Epoch 140 ...\n",
            "Running Epoch 141 ...\n",
            "Running Epoch 142 ...\n",
            "Running Epoch 143 ...\n",
            "Running Epoch 144 ...\n",
            "Running Epoch 145 ...\n",
            "Running Epoch 146 ...\n",
            "Running Epoch 147 ...\n",
            "Running Epoch 148 ...\n",
            "Running Epoch 149 ...\n",
            "Running Epoch 150 ...\n",
            "Running Epoch 151 ...\n",
            "Running Epoch 152 ...\n",
            "Running Epoch 153 ...\n",
            "Running Epoch 154 ...\n",
            "Running Epoch 155 ...\n",
            "Running Epoch 156 ...\n",
            "Running Epoch 157 ...\n",
            "Running Epoch 158 ...\n",
            "Running Epoch 159 ...\n",
            "Running Epoch 160 ...\n",
            "Running Epoch 161 ...\n",
            "Running Epoch 162 ...\n",
            "Running Epoch 163 ...\n",
            "Running Epoch 164 ...\n",
            "Running Epoch 165 ...\n",
            "Running Epoch 166 ...\n",
            "Running Epoch 167 ...\n",
            "Running Epoch 168 ...\n",
            "Running Epoch 169 ...\n",
            "Running Epoch 170 ...\n",
            "Running Epoch 171 ...\n",
            "Running Epoch 172 ...\n",
            "Running Epoch 173 ...\n",
            "Running Epoch 174 ...\n",
            "Running Epoch 175 ...\n",
            "Running Epoch 176 ...\n",
            "Running Epoch 177 ...\n",
            "Running Epoch 178 ...\n",
            "Running Epoch 179 ...\n",
            "Running Epoch 180 ...\n",
            "Running Epoch 181 ...\n",
            "Running Epoch 182 ...\n",
            "Running Epoch 183 ...\n",
            "Running Epoch 184 ...\n",
            "Running Epoch 185 ...\n",
            "Running Epoch 186 ...\n",
            "Running Epoch 187 ...\n",
            "Running Epoch 188 ...\n",
            "Running Epoch 189 ...\n",
            "Running Epoch 190 ...\n",
            "Running Epoch 191 ...\n",
            "Running Epoch 192 ...\n",
            "Running Epoch 193 ...\n",
            "Running Epoch 194 ...\n",
            "Running Epoch 195 ...\n",
            "Running Epoch 196 ...\n",
            "Running Epoch 197 ...\n",
            "Running Epoch 198 ...\n",
            "Running Epoch 199 ...\n",
            "Running Epoch 200 ...\n",
            "Running Epoch 201 ...\n",
            "Running Epoch 202 ...\n",
            "Running Epoch 203 ...\n",
            "Running Epoch 204 ...\n",
            "Running Epoch 205 ...\n",
            "Running Epoch 206 ...\n",
            "Running Epoch 207 ...\n",
            "Running Epoch 208 ...\n",
            "Running Epoch 209 ...\n",
            "Running Epoch 210 ...\n",
            "Running Epoch 211 ...\n",
            "Running Epoch 212 ...\n",
            "Running Epoch 213 ...\n",
            "Running Epoch 214 ...\n",
            "Running Epoch 215 ...\n",
            "Running Epoch 216 ...\n",
            "Running Epoch 217 ...\n",
            "Running Epoch 218 ...\n",
            "Running Epoch 219 ...\n",
            "Running Epoch 220 ...\n",
            "Running Epoch 221 ...\n",
            "Running Epoch 222 ...\n",
            "Running Epoch 223 ...\n",
            "Running Epoch 224 ...\n",
            "Running Epoch 225 ...\n",
            "Running Epoch 226 ...\n",
            "Running Epoch 227 ...\n",
            "Running Epoch 228 ...\n",
            "Running Epoch 229 ...\n",
            "Running Epoch 230 ...\n",
            "Running Epoch 231 ...\n",
            "Running Epoch 232 ...\n",
            "Running Epoch 233 ...\n",
            "Running Epoch 234 ...\n",
            "Running Epoch 235 ...\n",
            "Running Epoch 236 ...\n",
            "Running Epoch 237 ...\n",
            "Running Epoch 238 ...\n",
            "Running Epoch 239 ...\n",
            "Running Epoch 240 ...\n",
            "Running Epoch 241 ...\n",
            "Running Epoch 242 ...\n",
            "Running Epoch 243 ...\n",
            "Running Epoch 244 ...\n",
            "Running Epoch 245 ...\n",
            "Running Epoch 246 ...\n",
            "Running Epoch 247 ...\n",
            "Running Epoch 248 ...\n",
            "Running Epoch 249 ...\n",
            "Running Epoch 250 ...\n",
            "Running Epoch 251 ...\n",
            "Running Epoch 252 ...\n",
            "Running Epoch 253 ...\n",
            "Running Epoch 254 ...\n",
            "Running Epoch 255 ...\n",
            "Running Epoch 256 ...\n",
            "Running Epoch 257 ...\n",
            "Running Epoch 258 ...\n",
            "Running Epoch 259 ...\n",
            "Running Epoch 260 ...\n",
            "Running Epoch 261 ...\n",
            "Running Epoch 262 ...\n",
            "Running Epoch 263 ...\n",
            "Running Epoch 264 ...\n",
            "Running Epoch 265 ...\n",
            "Running Epoch 266 ...\n",
            "Running Epoch 267 ...\n",
            "Running Epoch 268 ...\n",
            "Running Epoch 269 ...\n",
            "Running Epoch 270 ...\n",
            "Running Epoch 271 ...\n",
            "Running Epoch 272 ...\n",
            "Running Epoch 273 ...\n",
            "Running Epoch 274 ...\n",
            "Running Epoch 275 ...\n",
            "Running Epoch 276 ...\n",
            "Running Epoch 277 ...\n",
            "Running Epoch 278 ...\n",
            "Running Epoch 279 ...\n",
            "Running Epoch 280 ...\n",
            "Running Epoch 281 ...\n",
            "Running Epoch 282 ...\n",
            "Running Epoch 283 ...\n",
            "Running Epoch 284 ...\n",
            "Running Epoch 285 ...\n",
            "Running Epoch 286 ...\n",
            "Running Epoch 287 ...\n",
            "Running Epoch 288 ...\n",
            "Running Epoch 289 ...\n",
            "Running Epoch 290 ...\n",
            "Running Epoch 291 ...\n",
            "Running Epoch 292 ...\n",
            "Running Epoch 293 ...\n",
            "Running Epoch 294 ...\n",
            "Running Epoch 295 ...\n",
            "Running Epoch 296 ...\n",
            "Running Epoch 297 ...\n",
            "Running Epoch 298 ...\n",
            "Running Epoch 299 ...\n",
            "Running Epoch 300 ...\n",
            "Running Epoch 301 ...\n",
            "Running Epoch 302 ...\n",
            "Running Epoch 303 ...\n",
            "Running Epoch 304 ...\n",
            "Running Epoch 305 ...\n",
            "Running Epoch 306 ...\n",
            "Running Epoch 307 ...\n",
            "Running Epoch 308 ...\n",
            "Running Epoch 309 ...\n",
            "Running Epoch 310 ...\n",
            "Running Epoch 311 ...\n",
            "Running Epoch 312 ...\n",
            "Running Epoch 313 ...\n",
            "Running Epoch 314 ...\n",
            "Running Epoch 315 ...\n",
            "Running Epoch 316 ...\n",
            "Running Epoch 317 ...\n",
            "Running Epoch 318 ...\n",
            "Running Epoch 319 ...\n",
            "Running Epoch 320 ...\n",
            "Running Epoch 321 ...\n",
            "Running Epoch 322 ...\n",
            "Running Epoch 323 ...\n",
            "Running Epoch 324 ...\n",
            "Running Epoch 325 ...\n",
            "Running Epoch 326 ...\n",
            "Running Epoch 327 ...\n",
            "Running Epoch 328 ...\n",
            "Running Epoch 329 ...\n",
            "Running Epoch 330 ...\n",
            "Running Epoch 331 ...\n",
            "Running Epoch 332 ...\n",
            "Running Epoch 333 ...\n",
            "Running Epoch 334 ...\n",
            "Running Epoch 335 ...\n",
            "Running Epoch 336 ...\n",
            "Running Epoch 337 ...\n",
            "Running Epoch 338 ...\n",
            "Running Epoch 339 ...\n",
            "Running Epoch 340 ...\n",
            "Running Epoch 341 ...\n",
            "Running Epoch 342 ...\n",
            "Running Epoch 343 ...\n",
            "Running Epoch 344 ...\n",
            "Running Epoch 345 ...\n",
            "Running Epoch 346 ...\n",
            "Running Epoch 347 ...\n",
            "Running Epoch 348 ...\n",
            "Running Epoch 349 ...\n",
            "Running Epoch 350 ...\n",
            "Running Epoch 351 ...\n",
            "Running Epoch 352 ...\n",
            "Running Epoch 353 ...\n",
            "Running Epoch 354 ...\n",
            "Running Epoch 355 ...\n",
            "Running Epoch 356 ...\n",
            "Running Epoch 357 ...\n",
            "Running Epoch 358 ...\n",
            "Running Epoch 359 ...\n",
            "Running Epoch 360 ...\n",
            "Running Epoch 361 ...\n",
            "Running Epoch 362 ...\n",
            "Running Epoch 363 ...\n",
            "Running Epoch 364 ...\n",
            "Running Epoch 365 ...\n",
            "Running Epoch 366 ...\n",
            "Running Epoch 367 ...\n",
            "Running Epoch 368 ...\n",
            "Running Epoch 369 ...\n",
            "Running Epoch 370 ...\n",
            "Running Epoch 371 ...\n",
            "Running Epoch 372 ...\n",
            "Running Epoch 373 ...\n",
            "Running Epoch 374 ...\n",
            "Running Epoch 375 ...\n",
            "Running Epoch 376 ...\n",
            "Running Epoch 377 ...\n",
            "Running Epoch 378 ...\n",
            "Running Epoch 379 ...\n",
            "Running Epoch 380 ...\n",
            "Running Epoch 381 ...\n",
            "Running Epoch 382 ...\n",
            "Running Epoch 383 ...\n",
            "Running Epoch 384 ...\n",
            "Running Epoch 385 ...\n",
            "Running Epoch 386 ...\n",
            "Running Epoch 387 ...\n",
            "Running Epoch 388 ...\n",
            "Running Epoch 389 ...\n",
            "Running Epoch 390 ...\n",
            "Running Epoch 391 ...\n",
            "Running Epoch 392 ...\n",
            "Running Epoch 393 ...\n",
            "Running Epoch 394 ...\n",
            "Running Epoch 395 ...\n",
            "Running Epoch 396 ...\n",
            "Running Epoch 397 ...\n",
            "Running Epoch 398 ...\n",
            "Running Epoch 399 ...\n",
            "Running Epoch 400 ...\n",
            "Running Epoch 401 ...\n",
            "Running Epoch 402 ...\n",
            "Running Epoch 403 ...\n",
            "Running Epoch 404 ...\n",
            "Running Epoch 405 ...\n",
            "Running Epoch 406 ...\n",
            "Running Epoch 407 ...\n",
            "Running Epoch 408 ...\n",
            "Running Epoch 409 ...\n",
            "Running Epoch 410 ...\n",
            "Running Epoch 411 ...\n",
            "Running Epoch 412 ...\n",
            "Running Epoch 413 ...\n",
            "Running Epoch 414 ...\n",
            "Running Epoch 415 ...\n",
            "Running Epoch 416 ...\n",
            "Running Epoch 417 ...\n",
            "Running Epoch 418 ...\n",
            "Running Epoch 419 ...\n",
            "Running Epoch 420 ...\n",
            "Running Epoch 421 ...\n",
            "Running Epoch 422 ...\n",
            "Running Epoch 423 ...\n",
            "Running Epoch 424 ...\n",
            "Running Epoch 425 ...\n",
            "Running Epoch 426 ...\n",
            "Running Epoch 427 ...\n",
            "Running Epoch 428 ...\n",
            "Running Epoch 429 ...\n",
            "Running Epoch 430 ...\n",
            "Running Epoch 431 ...\n",
            "Running Epoch 432 ...\n",
            "Running Epoch 433 ...\n",
            "Running Epoch 434 ...\n",
            "Running Epoch 435 ...\n",
            "Running Epoch 436 ...\n",
            "Running Epoch 437 ...\n",
            "Running Epoch 438 ...\n",
            "Running Epoch 439 ...\n",
            "Running Epoch 440 ...\n",
            "Running Epoch 441 ...\n",
            "Running Epoch 442 ...\n",
            "Running Epoch 443 ...\n",
            "Running Epoch 444 ...\n",
            "Running Epoch 445 ...\n",
            "Running Epoch 446 ...\n",
            "Running Epoch 447 ...\n",
            "Running Epoch 448 ...\n",
            "Running Epoch 449 ...\n",
            "Running Epoch 450 ...\n",
            "Running Epoch 451 ...\n",
            "Running Epoch 452 ...\n",
            "Running Epoch 453 ...\n",
            "Running Epoch 454 ...\n",
            "Running Epoch 455 ...\n",
            "Running Epoch 456 ...\n",
            "Running Epoch 457 ...\n",
            "Running Epoch 458 ...\n",
            "Running Epoch 459 ...\n",
            "Running Epoch 460 ...\n",
            "Running Epoch 461 ...\n",
            "Running Epoch 462 ...\n",
            "Running Epoch 463 ...\n",
            "Running Epoch 464 ...\n",
            "Running Epoch 465 ...\n",
            "Running Epoch 466 ...\n",
            "Running Epoch 467 ...\n",
            "Running Epoch 468 ...\n",
            "Running Epoch 469 ...\n",
            "Running Epoch 470 ...\n",
            "Running Epoch 471 ...\n",
            "Running Epoch 472 ...\n",
            "Running Epoch 473 ...\n",
            "Running Epoch 474 ...\n",
            "Running Epoch 475 ...\n",
            "Running Epoch 476 ...\n",
            "Running Epoch 477 ...\n",
            "Running Epoch 478 ...\n",
            "Running Epoch 479 ...\n",
            "Running Epoch 480 ...\n",
            "Running Epoch 481 ...\n",
            "Running Epoch 482 ...\n",
            "Running Epoch 483 ...\n",
            "Running Epoch 484 ...\n",
            "Running Epoch 485 ...\n",
            "Running Epoch 486 ...\n",
            "Running Epoch 487 ...\n",
            "Running Epoch 488 ...\n",
            "Running Epoch 489 ...\n",
            "Running Epoch 490 ...\n",
            "Running Epoch 491 ...\n",
            "Running Epoch 492 ...\n",
            "Running Epoch 493 ...\n",
            "Running Epoch 494 ...\n",
            "Running Epoch 495 ...\n",
            "Running Epoch 496 ...\n",
            "Running Epoch 497 ...\n",
            "Running Epoch 498 ...\n",
            "Running Epoch 499 ...\n",
            "Running Epoch 500 ...\n",
            "Running Epoch 501 ...\n",
            "Running Epoch 502 ...\n",
            "Running Epoch 503 ...\n",
            "Running Epoch 504 ...\n",
            "Running Epoch 505 ...\n",
            "Running Epoch 506 ...\n",
            "Running Epoch 507 ...\n",
            "Running Epoch 508 ...\n",
            "Running Epoch 509 ...\n",
            "Running Epoch 510 ...\n",
            "Running Epoch 511 ...\n",
            "Running Epoch 512 ...\n",
            "Running Epoch 513 ...\n",
            "Running Epoch 514 ...\n",
            "Running Epoch 515 ...\n",
            "Running Epoch 516 ...\n",
            "Running Epoch 517 ...\n",
            "Running Epoch 518 ...\n",
            "Running Epoch 519 ...\n",
            "Running Epoch 520 ...\n",
            "Running Epoch 521 ...\n",
            "Running Epoch 522 ...\n",
            "Running Epoch 523 ...\n",
            "Running Epoch 524 ...\n",
            "Running Epoch 525 ...\n",
            "Running Epoch 526 ...\n",
            "Running Epoch 527 ...\n",
            "Running Epoch 528 ...\n",
            "Running Epoch 529 ...\n",
            "Running Epoch 530 ...\n",
            "Running Epoch 531 ...\n",
            "Running Epoch 532 ...\n",
            "Running Epoch 533 ...\n",
            "Running Epoch 534 ...\n",
            "Running Epoch 535 ...\n",
            "Running Epoch 536 ...\n",
            "Running Epoch 537 ...\n",
            "Running Epoch 538 ...\n",
            "Running Epoch 539 ...\n",
            "Running Epoch 540 ...\n",
            "Running Epoch 541 ...\n",
            "Running Epoch 542 ...\n",
            "Running Epoch 543 ...\n",
            "Running Epoch 544 ...\n",
            "Running Epoch 545 ...\n",
            "Running Epoch 546 ...\n",
            "Running Epoch 547 ...\n",
            "Running Epoch 548 ...\n",
            "Running Epoch 549 ...\n",
            "Running Epoch 550 ...\n",
            "Running Epoch 551 ...\n",
            "Running Epoch 552 ...\n",
            "Running Epoch 553 ...\n",
            "Running Epoch 554 ...\n",
            "Running Epoch 555 ...\n",
            "Running Epoch 556 ...\n",
            "Running Epoch 557 ...\n",
            "Running Epoch 558 ...\n",
            "Running Epoch 559 ...\n",
            "Running Epoch 560 ...\n",
            "Running Epoch 561 ...\n",
            "Running Epoch 562 ...\n",
            "Running Epoch 563 ...\n",
            "Running Epoch 564 ...\n",
            "Running Epoch 565 ...\n",
            "Running Epoch 566 ...\n",
            "Running Epoch 567 ...\n",
            "Running Epoch 568 ...\n",
            "Running Epoch 569 ...\n",
            "Running Epoch 570 ...\n",
            "Running Epoch 571 ...\n",
            "Running Epoch 572 ...\n",
            "Running Epoch 573 ...\n",
            "Running Epoch 574 ...\n",
            "Running Epoch 575 ...\n",
            "Running Epoch 576 ...\n",
            "Running Epoch 577 ...\n",
            "Running Epoch 578 ...\n",
            "Running Epoch 579 ...\n",
            "Running Epoch 580 ...\n",
            "Running Epoch 581 ...\n",
            "Running Epoch 582 ...\n",
            "Running Epoch 583 ...\n",
            "Running Epoch 584 ...\n",
            "Running Epoch 585 ...\n",
            "Running Epoch 586 ...\n",
            "Running Epoch 587 ...\n",
            "Running Epoch 588 ...\n",
            "Running Epoch 589 ...\n",
            "Running Epoch 590 ...\n",
            "Running Epoch 591 ...\n",
            "Running Epoch 592 ...\n",
            "Running Epoch 593 ...\n",
            "Running Epoch 594 ...\n",
            "Running Epoch 595 ...\n",
            "Running Epoch 596 ...\n",
            "Running Epoch 597 ...\n",
            "Running Epoch 598 ...\n",
            "Running Epoch 599 ...\n",
            "Running Epoch 600 ...\n",
            "Running Epoch 601 ...\n",
            "Running Epoch 602 ...\n",
            "Running Epoch 603 ...\n",
            "Running Epoch 604 ...\n",
            "Running Epoch 605 ...\n",
            "Running Epoch 606 ...\n",
            "Running Epoch 607 ...\n",
            "Running Epoch 608 ...\n",
            "Running Epoch 609 ...\n",
            "Running Epoch 610 ...\n",
            "Running Epoch 611 ...\n",
            "Running Epoch 612 ...\n",
            "Running Epoch 613 ...\n",
            "Running Epoch 614 ...\n",
            "Running Epoch 615 ...\n",
            "Running Epoch 616 ...\n",
            "Running Epoch 617 ...\n",
            "Running Epoch 618 ...\n",
            "Running Epoch 619 ...\n",
            "Running Epoch 620 ...\n",
            "Running Epoch 621 ...\n",
            "Running Epoch 622 ...\n",
            "Running Epoch 623 ...\n",
            "Running Epoch 624 ...\n",
            "Running Epoch 625 ...\n",
            "Running Epoch 626 ...\n",
            "Running Epoch 627 ...\n",
            "Running Epoch 628 ...\n",
            "Running Epoch 629 ...\n",
            "Running Epoch 630 ...\n",
            "Running Epoch 631 ...\n",
            "Running Epoch 632 ...\n",
            "Running Epoch 633 ...\n",
            "Running Epoch 634 ...\n",
            "Running Epoch 635 ...\n",
            "Running Epoch 636 ...\n",
            "Running Epoch 637 ...\n",
            "Running Epoch 638 ...\n",
            "Running Epoch 639 ...\n",
            "Running Epoch 640 ...\n",
            "Running Epoch 641 ...\n",
            "Running Epoch 642 ...\n",
            "Running Epoch 643 ...\n",
            "Running Epoch 644 ...\n",
            "Running Epoch 645 ...\n",
            "Running Epoch 646 ...\n",
            "Running Epoch 647 ...\n",
            "Running Epoch 648 ...\n",
            "Running Epoch 649 ...\n",
            "Running Epoch 650 ...\n",
            "Running Epoch 651 ...\n",
            "Running Epoch 652 ...\n",
            "Running Epoch 653 ...\n",
            "Running Epoch 654 ...\n",
            "Running Epoch 655 ...\n",
            "Running Epoch 656 ...\n",
            "Running Epoch 657 ...\n",
            "Running Epoch 658 ...\n",
            "Running Epoch 659 ...\n",
            "Running Epoch 660 ...\n",
            "Running Epoch 661 ...\n",
            "Running Epoch 662 ...\n",
            "Running Epoch 663 ...\n",
            "Running Epoch 664 ...\n",
            "Running Epoch 665 ...\n",
            "Running Epoch 666 ...\n",
            "Running Epoch 667 ...\n",
            "Running Epoch 668 ...\n",
            "Running Epoch 669 ...\n",
            "Running Epoch 670 ...\n",
            "Running Epoch 671 ...\n",
            "Running Epoch 672 ...\n",
            "Running Epoch 673 ...\n",
            "Running Epoch 674 ...\n",
            "Running Epoch 675 ...\n",
            "Running Epoch 676 ...\n",
            "Running Epoch 677 ...\n",
            "Running Epoch 678 ...\n",
            "Running Epoch 679 ...\n",
            "Running Epoch 680 ...\n",
            "Running Epoch 681 ...\n",
            "Running Epoch 682 ...\n",
            "Running Epoch 683 ...\n",
            "Running Epoch 684 ...\n",
            "Running Epoch 685 ...\n",
            "Running Epoch 686 ...\n",
            "Running Epoch 687 ...\n",
            "Running Epoch 688 ...\n",
            "Running Epoch 689 ...\n",
            "Running Epoch 690 ...\n",
            "Running Epoch 691 ...\n",
            "Running Epoch 692 ...\n",
            "Running Epoch 693 ...\n",
            "Running Epoch 694 ...\n",
            "Running Epoch 695 ...\n",
            "Running Epoch 696 ...\n",
            "Running Epoch 697 ...\n",
            "Running Epoch 698 ...\n",
            "Running Epoch 699 ...\n",
            "Running Epoch 700 ...\n",
            "Running Epoch 701 ...\n",
            "Running Epoch 702 ...\n",
            "Running Epoch 703 ...\n",
            "Running Epoch 704 ...\n",
            "Running Epoch 705 ...\n",
            "Running Epoch 706 ...\n",
            "Running Epoch 707 ...\n",
            "Running Epoch 708 ...\n",
            "Running Epoch 709 ...\n",
            "Running Epoch 710 ...\n",
            "Running Epoch 711 ...\n",
            "Running Epoch 712 ...\n",
            "Running Epoch 713 ...\n",
            "Running Epoch 714 ...\n",
            "Running Epoch 715 ...\n",
            "Running Epoch 716 ...\n",
            "Running Epoch 717 ...\n",
            "Running Epoch 718 ...\n",
            "Running Epoch 719 ...\n",
            "Running Epoch 720 ...\n",
            "Running Epoch 721 ...\n",
            "Running Epoch 722 ...\n",
            "Running Epoch 723 ...\n",
            "Running Epoch 724 ...\n",
            "Running Epoch 725 ...\n",
            "Running Epoch 726 ...\n",
            "Running Epoch 727 ...\n",
            "Running Epoch 728 ...\n",
            "Running Epoch 729 ...\n",
            "Running Epoch 730 ...\n",
            "Running Epoch 731 ...\n",
            "Running Epoch 732 ...\n",
            "Running Epoch 733 ...\n",
            "Running Epoch 734 ...\n",
            "Running Epoch 735 ...\n",
            "Running Epoch 736 ...\n",
            "Running Epoch 737 ...\n",
            "Running Epoch 738 ...\n",
            "Running Epoch 739 ...\n",
            "Running Epoch 740 ...\n",
            "Running Epoch 741 ...\n",
            "Running Epoch 742 ...\n",
            "Running Epoch 743 ...\n",
            "Running Epoch 744 ...\n",
            "Running Epoch 745 ...\n",
            "Running Epoch 746 ...\n",
            "Running Epoch 747 ...\n",
            "Running Epoch 748 ...\n",
            "Running Epoch 749 ...\n",
            "Running Epoch 750 ...\n",
            "Running Epoch 751 ...\n",
            "Running Epoch 752 ...\n",
            "Running Epoch 753 ...\n",
            "Running Epoch 754 ...\n",
            "Running Epoch 755 ...\n",
            "Running Epoch 756 ...\n",
            "Running Epoch 757 ...\n",
            "Running Epoch 758 ...\n",
            "Running Epoch 759 ...\n",
            "Running Epoch 760 ...\n",
            "Running Epoch 761 ...\n",
            "Running Epoch 762 ...\n",
            "Running Epoch 763 ...\n",
            "Running Epoch 764 ...\n",
            "Running Epoch 765 ...\n",
            "Running Epoch 766 ...\n",
            "Running Epoch 767 ...\n",
            "Running Epoch 768 ...\n",
            "Running Epoch 769 ...\n",
            "Running Epoch 770 ...\n",
            "Running Epoch 771 ...\n",
            "Running Epoch 772 ...\n",
            "Running Epoch 773 ...\n",
            "Running Epoch 774 ...\n",
            "Running Epoch 775 ...\n",
            "Running Epoch 776 ...\n",
            "Running Epoch 777 ...\n",
            "Running Epoch 778 ...\n",
            "Running Epoch 779 ...\n",
            "Running Epoch 780 ...\n",
            "Running Epoch 781 ...\n",
            "Running Epoch 782 ...\n",
            "Running Epoch 783 ...\n",
            "Running Epoch 784 ...\n",
            "Running Epoch 785 ...\n",
            "Running Epoch 786 ...\n",
            "Running Epoch 787 ...\n",
            "Running Epoch 789 ...\n",
            "Running Epoch 790 ...\n",
            "Running Epoch 791 ...\n",
            "Running Epoch 792 ...\n",
            "Running Epoch 793 ...\n",
            "Running Epoch 794 ...\n",
            "Running Epoch 795 ...\n",
            "Running Epoch 796 ...\n",
            "Running Epoch 797 ...\n",
            "Running Epoch 798 ...\n",
            "Running Epoch 799 ...\n",
            "Running Epoch 800 ...\n",
            "Running Epoch 801 ...\n",
            "Running Epoch 802 ...\n",
            "Running Epoch 803 ...\n",
            "Running Epoch 804 ...\n",
            "Running Epoch 805 ...\n",
            "Running Epoch 806 ...\n",
            "Running Epoch 807 ...\n",
            "Running Epoch 808 ...\n",
            "Running Epoch 809 ...\n",
            "Running Epoch 810 ...\n",
            "Running Epoch 811 ...\n",
            "Running Epoch 812 ...\n",
            "Running Epoch 813 ...\n",
            "Running Epoch 814 ...\n",
            "Running Epoch 815 ...\n",
            "Running Epoch 816 ...\n",
            "Running Epoch 817 ...\n",
            "Running Epoch 818 ...\n",
            "Running Epoch 819 ...\n",
            "Running Epoch 820 ...\n",
            "Running Epoch 821 ...\n",
            "Running Epoch 822 ...\n",
            "Running Epoch 823 ...\n",
            "Running Epoch 824 ...\n",
            "Running Epoch 825 ...\n",
            "Running Epoch 826 ...\n",
            "Running Epoch 827 ...\n",
            "Running Epoch 828 ...\n",
            "Running Epoch 829 ...\n",
            "Running Epoch 830 ...\n",
            "Running Epoch 831 ...\n",
            "Running Epoch 832 ...\n",
            "Running Epoch 833 ...\n",
            "Running Epoch 834 ...\n",
            "Running Epoch 835 ...\n",
            "Running Epoch 836 ...\n",
            "Running Epoch 837 ...\n",
            "Running Epoch 838 ...\n",
            "Running Epoch 839 ...\n",
            "Running Epoch 840 ...\n",
            "Running Epoch 841 ...\n",
            "Running Epoch 842 ...\n",
            "Running Epoch 843 ...\n",
            "Running Epoch 844 ...\n",
            "Running Epoch 845 ...\n",
            "Running Epoch 846 ...\n",
            "Running Epoch 847 ...\n",
            "Running Epoch 848 ...\n",
            "Running Epoch 849 ...\n",
            "Running Epoch 850 ...\n",
            "Running Epoch 851 ...\n",
            "Running Epoch 852 ...\n",
            "Running Epoch 853 ...\n",
            "Running Epoch 854 ...\n",
            "Running Epoch 855 ...\n",
            "Running Epoch 856 ...\n",
            "Running Epoch 857 ...\n",
            "Running Epoch 858 ...\n",
            "Running Epoch 859 ...\n",
            "Running Epoch 860 ...\n",
            "Running Epoch 861 ...\n",
            "Running Epoch 862 ...\n",
            "Running Epoch 863 ...\n",
            "Running Epoch 864 ...\n",
            "Running Epoch 865 ...\n",
            "Running Epoch 866 ...\n",
            "Running Epoch 867 ...\n",
            "Running Epoch 868 ...\n",
            "Running Epoch 869 ...\n",
            "Running Epoch 870 ...\n",
            "Running Epoch 871 ...\n",
            "Running Epoch 872 ...\n",
            "Running Epoch 873 ...\n",
            "Running Epoch 874 ...\n",
            "Running Epoch 875 ...\n",
            "Running Epoch 876 ...\n",
            "Running Epoch 877 ...\n",
            "Running Epoch 878 ...\n",
            "Running Epoch 879 ...\n",
            "Running Epoch 880 ...\n",
            "Running Epoch 881 ...\n",
            "Running Epoch 882 ...\n",
            "Running Epoch 883 ...\n",
            "Running Epoch 884 ...\n",
            "Running Epoch 885 ...\n",
            "Running Epoch 886 ...\n",
            "Running Epoch 887 ...\n",
            "Running Epoch 888 ...\n",
            "Running Epoch 889 ...\n",
            "Running Epoch 890 ...\n",
            "Running Epoch 891 ...\n",
            "Running Epoch 892 ...\n",
            "Running Epoch 893 ...\n",
            "Running Epoch 894 ...\n",
            "Running Epoch 895 ...\n",
            "Running Epoch 896 ...\n",
            "Running Epoch 897 ...\n",
            "Running Epoch 898 ...\n",
            "Running Epoch 899 ...\n",
            "Running Epoch 900 ...\n",
            "Running Epoch 901 ...\n",
            "Running Epoch 902 ...\n",
            "Running Epoch 903 ...\n",
            "Running Epoch 904 ...\n",
            "Running Epoch 905 ...\n",
            "Running Epoch 906 ...\n",
            "Running Epoch 907 ...\n",
            "Running Epoch 908 ...\n",
            "Running Epoch 909 ...\n",
            "Running Epoch 910 ...\n",
            "Running Epoch 911 ...\n",
            "Running Epoch 912 ...\n",
            "Running Epoch 913 ...\n",
            "Running Epoch 914 ...\n",
            "Running Epoch 915 ...\n",
            "Running Epoch 916 ...\n",
            "Running Epoch 917 ...\n",
            "Running Epoch 918 ...\n",
            "Running Epoch 919 ...\n",
            "Running Epoch 920 ...\n",
            "Running Epoch 921 ...\n",
            "Running Epoch 922 ...\n",
            "Running Epoch 923 ...\n",
            "Running Epoch 924 ...\n",
            "Running Epoch 925 ...\n",
            "Running Epoch 926 ...\n",
            "Running Epoch 927 ...\n",
            "Running Epoch 928 ...\n",
            "Running Epoch 929 ...\n",
            "Running Epoch 930 ...\n",
            "Running Epoch 931 ...\n",
            "Running Epoch 932 ...\n",
            "Running Epoch 933 ...\n",
            "Running Epoch 934 ...\n",
            "Running Epoch 935 ...\n",
            "Running Epoch 936 ...\n",
            "Running Epoch 937 ...\n",
            "Running Epoch 938 ...\n",
            "Running Epoch 939 ...\n",
            "Running Epoch 940 ...\n",
            "Running Epoch 941 ...\n",
            "Running Epoch 942 ...\n",
            "Running Epoch 943 ...\n",
            "Running Epoch 944 ...\n",
            "Running Epoch 945 ...\n",
            "Running Epoch 946 ...\n",
            "Running Epoch 947 ...\n",
            "Running Epoch 948 ...\n",
            "Running Epoch 949 ...\n",
            "Running Epoch 950 ...\n",
            "Running Epoch 951 ...\n",
            "Running Epoch 952 ...\n",
            "Running Epoch 953 ...\n",
            "Running Epoch 954 ...\n",
            "Running Epoch 955 ...\n",
            "Running Epoch 956 ...\n",
            "Running Epoch 957 ...\n",
            "Running Epoch 958 ...\n",
            "Running Epoch 959 ...\n",
            "Running Epoch 960 ...\n",
            "Running Epoch 961 ...\n",
            "Running Epoch 962 ...\n",
            "Running Epoch 963 ...\n",
            "Running Epoch 964 ...\n",
            "Running Epoch 965 ...\n",
            "Running Epoch 966 ...\n",
            "Running Epoch 967 ...\n",
            "Running Epoch 968 ...\n",
            "Running Epoch 969 ...\n",
            "Running Epoch 970 ...\n",
            "Running Epoch 971 ...\n",
            "Running Epoch 972 ...\n",
            "Running Epoch 973 ...\n",
            "Running Epoch 974 ...\n",
            "Running Epoch 975 ...\n",
            "Running Epoch 976 ...\n",
            "Running Epoch 977 ...\n",
            "Running Epoch 978 ...\n",
            "Running Epoch 979 ...\n",
            "Running Epoch 980 ...\n",
            "Running Epoch 981 ...\n",
            "Running Epoch 982 ...\n",
            "Running Epoch 983 ...\n",
            "Running Epoch 984 ...\n",
            "Running Epoch 985 ...\n",
            "Running Epoch 986 ...\n",
            "Running Epoch 987 ...\n",
            "Running Epoch 988 ...\n",
            "Running Epoch 989 ...\n",
            "Running Epoch 990 ...\n",
            "Running Epoch 991 ...\n",
            "Running Epoch 992 ...\n",
            "Running Epoch 993 ...\n",
            "Running Epoch 994 ...\n",
            "Running Epoch 995 ...\n",
            "Running Epoch 996 ...\n",
            "Running Epoch 997 ...\n",
            "Running Epoch 998 ...\n",
            "Running Epoch 999 ...\n",
            "Finished training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming you have already prepared your evaluation data in the correct format\n",
        "\n",
        "def evaluate(model, data_loader, loss_function):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            # Move inputs and targets to the device (CPU or GPU)\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = loss_function(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate the number of correct predictions (assuming you are doing a classification task)\n",
        "            _, predicted_indices = torch.max(outputs, 1)\n",
        "            total_correct += (predicted_indices == targets).sum().item()\n",
        "\n",
        "    # Calculate the average loss and accuracy\n",
        "    average_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = total_correct / len(data_loader.dataset)\n",
        "\n",
        "    return average_loss, accuracy\n",
        "\n",
        "# Assuming you have already created your LanguageModel instance and loaded model parameters\n",
        "\n",
        "# Set your loss function (e.g., CrossEntropyLoss for classification tasks)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Assuming you have your evaluation data in a DataLoader\n",
        "\n",
        "eval_data_loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=128)  # Replace ... with your evaluation data loader\n"
      ],
      "metadata": {
        "id": "hLf7jyLNEMDE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "average_loss, accuracy = evaluate(model, eval_data_loader, loss_function)\n",
        "\n",
        "print(f\"1. Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57Ls8xrtFWK1",
        "outputId": "c15f6ff1-62b1-4348-ce57-7a3d71678163"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Average Loss: 0.0037, Accuracy: 0.9082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_promt(prompt):\n",
        "  prompt = prompt.split(' ')\n",
        "  prompt_processed = []\n",
        "  for word in prompt:\n",
        "    if word in words_unique:\n",
        "      prompt_processed.append(word)\n",
        "    else:\n",
        "      for c in word:\n",
        "        if c in words_unique:\n",
        "          prompt_processed.append(c)\n",
        "    prompt_processed.append(' ')\n",
        "  prompt_processed = [word for word in prompt_processed if word != '']\n",
        "  return prompt_processed"
      ],
      "metadata": {
        "id": "6-FFrgHvFXnA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(prompt):\n",
        "    prompt = encode_promt(prompt)\n",
        "    sequence = [word_to_int[word] for word in prompt]\n",
        "    answer = ''\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(sequence)*3):\n",
        "            x = np.reshape(sequence, (1, len(sequence), 1)) / float(n_vocab) # Reshape and normalize\n",
        "            x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "            prediction = model(x)\n",
        "            index = int(prediction.argmax()) # Predict an array of n_vocab integers\n",
        "            answer += ' ' + int_to_word[index]\n",
        "            sequence.append(index) # Append the predicted integer into the current sequence\n",
        "            sequence = sequence[1:] # Remove the first integer from the sequence\n",
        "    return ' '.join([int_to_word[i] for i in sequence]), answer # Convert all the integers into characters"
      ],
      "metadata": {
        "id": "SgJ7o5wKFZF8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"\"\"Cảo thơm lần giở trước đèn. Phong tình có lục còn truyền sử xanh.\"\"\")[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "AdvjFNcBFa2S",
        "outputId": "8e2bb11b-c308-4570-90d1-2e0c759b078e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' thế đã bà mày mày có với , - bà vẫn con vẫn với nhà . ông ð con con con nó con chưa giờ mới khóc ấy được làm . , không sen sộ cho - không nói câm sao cho đấy , ấy đây hãy ông , rồi nghị , lại được tha làm vào vào cho một , chị dậu tôi trộm bước cái ông giữ con lệ , bị làm lấy cái vào ý chồng lắm buồng - vâng làm ông cụ nhà đấy , rồi con chó ra cho chồng .\\n rồi nó lo chị dậu tiền , quan ở để cùng lí nhà anh xong , ra thêm ra sao hay'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using this, you do not need to define the model class again:\n",
        "\n",
        "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
        "model_scripted.save('/content/myModel.pt') # Save\n",
        "# Load\n",
        "#model = torch.jit.load('model_scripted.pt')\n",
        "#model.eval()"
      ],
      "metadata": {
        "id": "M8E8YJM9FcMw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/myModel.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cA4mVCsJFflC",
        "outputId": "5dd3e8c6-1956-4183-c6a4-20defaf780aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_74eff1e3-0f51-4dd4-bc5b-73ef973042e1\", \"myModel.pt\", 4990928)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "import gradio as gr\n",
        "\n",
        "model.eval() # Change to evaluation mode because we don't want Dropout Layer to automatically drop Neural Network nodes when we are making prediction\n",
        "def text_generation(prompt):\n",
        "    return predict(prompt)[1]\n",
        "gr.Interface(fn=text_generation, inputs=[\"text\"], outputs=[\"text\"]).launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzcfX4mNFhGM",
        "outputId": "2f15acaa-dddb-4c48-cf22-32c0ae982616"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-3.39.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.100.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.3.0 (from gradio)\n",
            "  Downloading gradio_client-0.3.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.12)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.7.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.3.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.6)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=9b1c9e7e522104b7b275ddeefaa66cc75e8d2ad84b20f26933153fdd40f230a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.100.1 ffmpy-0.3.1 gradio-3.39.0 gradio-client-0.3.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uvicorn-0.23.2 websockets-11.0.3\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e8019d256e1a82008e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e8019d256e1a82008e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "# File paths\n",
        "colab_file_path = '/content/myModel.pt'\n",
        "drive_file_path = '/content/drive/MyDrive/AI FPT/model_LSTM_for_5_tac_pham_Truyen_Kieu_Tat_den_HXH_Tho_Lao_Hac.pt'\n",
        "\n",
        "# Copy the file\n",
        "shutil.copyfile(colab_file_path, drive_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Vmtqep3oFu5T",
        "outputId": "f6d4fb2c-6c32-4631-925e-6e77812c9f55"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-60a8f5e2044f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1DLkie_GJyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}